{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcTL4f2G9WE"
      },
      "source": [
        "## Installation of libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet flashtext==2.7\n",
        "!pip install git+https://github.com/boudinfl/pke.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwa7uRpLodIC",
        "outputId": "6ac0fd29-37bb-4b3f-b142-0845a67c9164"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-lf6ip0ij\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-lf6ip0ij\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.7.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (0.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.3.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.3->pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.3->pke==2.0.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->pke==2.0.0) (2022.6.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==2.0.0) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pke==2.0.0) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM2h7czt2s0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b59010-12d3-422b-b9ae-c1995596b68b"
      },
      "source": [
        "!pip install --quiet transformers==4.8.1\n",
        "!pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2\n",
        "!pip install --quiet gradio==3.0.20"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 2.2.2 requires huggingface-hub>=0.4.0, but you have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet strsim==0.0.3\n",
        "!pip install --quiet sense2vec==2.0.0"
      ],
      "metadata": {
        "id": "2TfD1owPokS7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDHhiuKJ2eva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ca02a1-1c1c-4918-d408-79f15707fef8"
      },
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 449 µs (started: 2022-09-27 09:39:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet sentence-transformers==2.2.2"
      ],
      "metadata": {
        "id": "hfsjeHQmrKe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6d4118-3a6b-4c6b-eb78-af71c2674787"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.44 s (started: 2022-09-27 09:39:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdVBDYRGTAW7"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNhJ0wPn_kv"
      },
      "source": [
        "Text taken from: \n",
        "https://gadgets.ndtv.com/internet/news/dogecoin-price-rally-surge-elon-musk-tweet-twitter-working-developers-improve-transaction-efficiency-2442120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5wyqeq0Pq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4251b138-6908-418e-d5c7-25e37f389c12"
      },
      "source": [
        "from textwrap3 import wrap\n",
        "\n",
        "text = \"\"\"Indian Institute of Information Technology Sri City, Chittoor known as IIIT Sri City (IIITS) was established in 2013 by Ministry of Education, Government of India as an Institute of National importance by an Act of Parliament. IIITS is one among the 20 Institutes across India, focusing on Information Technology education, research and development.\n",
        "The institute is located in Sri City (www.sricity.in) located about 60 KMs from Chennai on the Nellore Highway. The nearest airports are Chennai (70 KMs) and Tirupati (70 KMs).\n",
        "IIITS The Institute was setup by the Government of India along with Government of Andhra Pradesh and Industry Partners represented by Sri City Foundation as a Public-Private-Partnership (PPP) institution. The Institute is governed by the Board of Governors consisting of eminent personalities from the Government, Industry and Academia. IIITS offers B. Tech, M.Tech, M.S. and Ph. D. programmes in the areas of Computer Science & Engineering and Electronics & Communications Engineering.\n",
        " \n",
        "IIITS significantly positioned as an institution known for UG-led research which aims to develop technology related research that can solve real-world problems and create a positive impact in our day-to-day lives. The Institute also offers students a unique opportunity to actively take part in research which would help create innovative and entrepreneurial mindset.\n",
        "Highly qualified and committed faculty members is the hallmark of IIITS. The fulltime faculty have received their PhD degrees from globally recognized universities. Many of them have spent couple of years’ time as Post-Doctoral Fellows in overseas universities. We at IIITS benchmark our pedagogy to leading universities and make continual efforts in upgrading our approach, course curriculum and teaching methodology. We emphasize and encourage students to be self-learners, gain experiential learning through involvement in various academic, research and co-curricular activities. The curriculum designed allows students to choose courses and projects of their choice. Moreover, the Institute promotes an environment wherein students are encouraged to participate in a variety of technical, cultural and social clubs, for skills development and community engagements.\n",
        "IIITS aims to be better year-by-year and make a difference to every stakeholder who is actively engaged with it. Our endeavor would be to benchmark ourselves with the Top 100 Technology Universities of the world in terms of infrastructure, technology, research, best practices, facilities, etc.. \"\"\"\n",
        "\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian Institute of Information Technology Sri City, Chittoor known as IIIT Sri City (IIITS) was established in 2013 by Ministry of Education,\n",
            "Government of India as an Institute of National importance by an Act of Parliament. IIITS is one among the 20 Institutes across India, focusing on\n",
            "Information Technology education, research and development. The institute is located in Sri City (www.sricity.in) located about 60 KMs from Chennai on\n",
            "the Nellore Highway. The nearest airports are Chennai (70 KMs) and Tirupati (70 KMs). IIITS The Institute was setup by the Government of India along\n",
            "with Government of Andhra Pradesh and Industry Partners represented by Sri City Foundation as a Public-Private-Partnership (PPP) institution. The\n",
            "Institute is governed by the Board of Governors consisting of eminent personalities from the Government, Industry and Academia. IIITS offers B. Tech,\n",
            "M.Tech, M.S. and Ph. D. programmes in the areas of Computer Science & Engineering and Electronics & Communications Engineering.   IIITS significantly\n",
            "positioned as an institution known for UG-led research which aims to develop technology related research that can solve real-world problems and create\n",
            "a positive impact in our day-to-day lives. The Institute also offers students a unique opportunity to actively take part in research which would help\n",
            "create innovative and entrepreneurial mindset. Highly qualified and committed faculty members is the hallmark of IIITS. The fulltime faculty have\n",
            "received their PhD degrees from globally recognized universities. Many of them have spent couple of years’ time as Post-Doctoral Fellows in overseas\n",
            "universities. We at IIITS benchmark our pedagogy to leading universities and make continual efforts in upgrading our approach, course curriculum and\n",
            "teaching methodology. We emphasize and encourage students to be self-learners, gain experiential learning through involvement in various academic,\n",
            "research and co-curricular activities. The curriculum designed allows students to choose courses and projects of their choice. Moreover, the Institute\n",
            "promotes an environment wherein students are encouraged to participate in a variety of technical, cultural and social clubs, for skills development\n",
            "and community engagements. IIITS aims to be better year-by-year and make a difference to every stakeholder who is actively engaged with it. Our\n",
            "endeavor would be to benchmark ourselves with the Top 100 Technology Universities of the world in terms of infrastructure, technology, research, best\n",
            "practices, facilities, etc..\n",
            "\n",
            "\n",
            "time: 10.3 ms (started: 2022-09-27 09:39:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScrtI0ueTFbR"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sHMJ5ubS5Gf"
      },
      "source": [
        "http://read.gov/aesop/007.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrMhx6BAP2Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcefd37-41e0-4747-bfe2-ffdb6ae34721"
      },
      "source": [
        "text = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence (AI). It helps machines process and understand the human language so that they can automatically perform repetitive tasks. Examples include machine translation, summarization, ticket classification, and spell check. Take sentiment analysis, for example, which uses natural language processing to detect emotions in text. This classification task is one of the most popular tasks of NLP, often used by businesses to automatically detect brand sentiment on social media. Analyzing these interactions can help brands detect urgent customer issues that they need to respond to right away, or monitor overall customer satisfaction. One of the main reasons natural language processing is so critical to businesses is that it can be used to analyze large volumes of text data, like social media comments, customer support tickets, online reviews, news reports, and more. All this business data contains a wealth of valuable insights, and NLP can quickly help businesses discover what those insights are. It does this by helping machines make sense of human language in a faster, more accurate, and more consistent way than human agents. NLP tools process data in real time, 24/7, and apply the same criteria to all your data, so you can ensure the results you receive are accurate – and not riddled with inconsistencies. Once NLP tools can understand what a piece of text is about, and even measure things like sentiment, businesses can start to prioritize and organize their data in a way that suits their needs. While there are many challenges in natural language processing, the benefits of NLP for businesses are huge making NLP a worthwhile investment. However, it’s important to know what those challenges are before getting started with NLP. Human language is complex, ambiguous, disorganized, and diverse. There are more than 6,500 languages in the world, all of them with their own syntactic and semantic rules. Even humans struggle to make sense of language. So for machines to understand natural language, it first needs to be transformed into something that they can interpret. In NLP, syntax and semantic analysis are key to understanding the grammatical structure of a text and identifying how words relate to each other in a given context. But, transforming text into something machines can process is complicated. Data scientists need to teach NLP tools to look beyond definitions and word order, to understand context, word ambiguities, and other complex concepts connected to human language.      \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI). It helps machines process and understand the human language so that\n",
            "they can automatically perform repetitive tasks. Examples include machine translation, summarization, ticket classification, and spell check. Take\n",
            "sentiment analysis, for example, which uses natural language processing to detect emotions in text. This classification task is one of the most\n",
            "popular tasks of NLP, often used by businesses to automatically detect brand sentiment on social media. Analyzing these interactions can help brands\n",
            "detect urgent customer issues that they need to respond to right away, or monitor overall customer satisfaction. One of the main reasons natural\n",
            "language processing is so critical to businesses is that it can be used to analyze large volumes of text data, like social media comments, customer\n",
            "support tickets, online reviews, news reports, and more. All this business data contains a wealth of valuable insights, and NLP can quickly help\n",
            "businesses discover what those insights are. It does this by helping machines make sense of human language in a faster, more accurate, and more\n",
            "consistent way than human agents. NLP tools process data in real time, 24/7, and apply the same criteria to all your data, so you can ensure the\n",
            "results you receive are accurate – and not riddled with inconsistencies. Once NLP tools can understand what a piece of text is about, and even measure\n",
            "things like sentiment, businesses can start to prioritize and organize their data in a way that suits their needs. While there are many challenges in\n",
            "natural language processing, the benefits of NLP for businesses are huge making NLP a worthwhile investment. However, it’s important to know what\n",
            "those challenges are before getting started with NLP. Human language is complex, ambiguous, disorganized, and diverse. There are more than 6,500\n",
            "languages in the world, all of them with their own syntactic and semantic rules. Even humans struggle to make sense of language. So for machines to\n",
            "understand natural language, it first needs to be transformed into something that they can interpret. In NLP, syntax and semantic analysis are key to\n",
            "understanding the grammatical structure of a text and identifying how words relate to each other in a given context. But, transforming text into\n",
            "something machines can process is complicated. Data scientists need to teach NLP tools to look beyond definitions and word order, to understand\n",
            "context, word ambiguities, and other complex concepts connected to human language.\n",
            "\n",
            "\n",
            "time: 2.47 ms (started: 2022-09-27 09:39:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbR470g15Fq"
      },
      "source": [
        "# **Summarization with T5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXs7fnvCarm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63eeb98-b932-49cd-ff82-f964609da26e"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary_model = summary_model.to(device)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.94 s (started: 2022-09-27 09:39:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzGsTUJ8TyAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0507eac-c353-4622-f3ae-f7f2c3a37511"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.51 ms (started: 2022-09-27 09:39:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JaEy5Xw_UMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650c6c69-2d7d-441f-cc7c-de98459c3075"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary\n",
        "\n",
        "\n",
        "summarized_text = summarizer(text,summary_model,summary_tokenizer)\n",
        "\n",
        "\n",
        "print (\"\\noriginal Text >>\")\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "print (\"Summarized Text >>\")\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "original Text >>\n",
            "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI). It helps machines process and understand the human language so that\n",
            "they can automatically perform repetitive tasks. Examples include machine translation, summarization, ticket classification, and spell check. Take\n",
            "sentiment analysis, for example, which uses natural language processing to detect emotions in text. This classification task is one of the most\n",
            "popular tasks of NLP, often used by businesses to automatically detect brand sentiment on social media. Analyzing these interactions can help brands\n",
            "detect urgent customer issues that they need to respond to right away, or monitor overall customer satisfaction. One of the main reasons natural\n",
            "language processing is so critical to businesses is that it can be used to analyze large volumes of text data, like social media comments, customer\n",
            "support tickets, online reviews, news reports, and more. All this business data contains a wealth of valuable insights, and NLP can quickly help\n",
            "businesses discover what those insights are. It does this by helping machines make sense of human language in a faster, more accurate, and more\n",
            "consistent way than human agents. NLP tools process data in real time, 24/7, and apply the same criteria to all your data, so you can ensure the\n",
            "results you receive are accurate – and not riddled with inconsistencies. Once NLP tools can understand what a piece of text is about, and even measure\n",
            "things like sentiment, businesses can start to prioritize and organize their data in a way that suits their needs. While there are many challenges in\n",
            "natural language processing, the benefits of NLP for businesses are huge making NLP a worthwhile investment. However, it’s important to know what\n",
            "those challenges are before getting started with NLP. Human language is complex, ambiguous, disorganized, and diverse. There are more than 6,500\n",
            "languages in the world, all of them with their own syntactic and semantic rules. Even humans struggle to make sense of language. So for machines to\n",
            "understand natural language, it first needs to be transformed into something that they can interpret. In NLP, syntax and semantic analysis are key to\n",
            "understanding the grammatical structure of a text and identifying how words relate to each other in a given context. But, transforming text into\n",
            "something machines can process is complicated. Data scientists need to teach NLP tools to look beyond definitions and word order, to understand\n",
            "context, word ambiguities, and other complex concepts connected to human language.\n",
            "\n",
            "\n",
            "Summarized Text >>\n",
            "Natural language processing (nlp) is a subfield of artificial intelligence (ai) it helps machines process and understand the human language so that\n",
            "they can automatically perform repetitive tasks. Examples include machine translation, summarization, ticket classification, and spell check.\n",
            "Sentiment analysis is one of the most popular tasks of nlp, often used by businesses to automatically detect brand sentiment on social media.\n",
            "\n",
            "\n",
            "time: 6.24 s (started: 2022-09-27 09:39:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0YrWTxQCo9q"
      },
      "source": [
        "# **Answer Span Extraction (Keywords and Noun Phrases)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84DxJGFn4MfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed73086-b708-4b92-d269-3e780d3ae4d6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content,language='en')\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        pos = {'PROPN','NOUN'}\n",
        "        #pos = {'PROPN','NOUN'}\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        extractor.candidate_selection(pos=pos)\n",
        "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "        #    threshold/method parameters.\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "        \n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return out"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.88 s (started: 2022-09-27 09:39:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_sRWHAd4Wwp",
        "outputId": "a3600180-d788-4065-f9cb-32ea44d9ca52"
      },
      "source": [
        "from flashtext import KeywordProcessor\n",
        "\n",
        "\n",
        "def get_keywords(originaltext,summarytext):\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  print (\"keywords unsummarized: \",keywords)\n",
        "  keyword_processor = KeywordProcessor()\n",
        "  for keyword in keywords:\n",
        "    keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  keywords_found = list(set(keywords_found))\n",
        "  print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  important_keywords =[]\n",
        "  for keyword in keywords:\n",
        "    if keyword in keywords_found:\n",
        "      important_keywords.append(keyword)\n",
        "\n",
        "  return important_keywords[:4]\n",
        "\n",
        "\n",
        "imp_keywords = get_keywords(text,summarized_text)\n",
        "print (imp_keywords)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keywords unsummarized:  ['natural language processing', 'nlp', 'businesses', 'machines process', 'text', 'language', 'tasks', 'brand sentiment', 'examples', 'subfield', 'customer issues', 'language processing', 'intelligence', 'media', 'sentiment analysis']\n",
            "keywords_found in summarized:  ['nlp', 'sentiment analysis', 'subfield', 'media', 'tasks', 'businesses', 'brand sentiment', 'examples', 'language', 'natural language processing', 'machines process', 'intelligence']\n",
            "['natural language processing', 'nlp', 'businesses', 'machines process']\n",
            "time: 765 ms (started: 2022-09-27 09:39:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbq7b2WCaZ_"
      },
      "source": [
        "# **Question generation with T5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CuKlpL1Cj6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a1bbdf-4771-44fd-ec47-216cc663a500"
      },
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_model = question_model.to(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.32 s (started: 2022-09-27 09:39:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uzA4uLJ_P48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c55659d-6ba9-4b78-cd3a-1a9f708c32f4"
      },
      "source": [
        "def get_question(context,answer,model,tokenizer):\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "\n",
        "for answer in imp_keywords:\n",
        "  ques = get_question(summarized_text,answer,question_model,question_tokenizer)\n",
        "  print (ques)\n",
        "  print (answer.capitalize())\n",
        "  print (\"\\n\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (nlp) is a subfield of artificial intelligence (ai) it helps machines process and understand the human language so that\n",
            "they can automatically perform repetitive tasks. Examples include machine translation, summarization, ticket classification, and spell check.\n",
            "Sentiment analysis is one of the most popular tasks of nlp, often used by businesses to automatically detect brand sentiment on social media.\n",
            "\n",
            "\n",
            "What does nlp stand for?\n",
            "Natural language processing\n",
            "\n",
            "\n",
            "What is a subfield of artificial intelligence called?\n",
            "Nlp\n",
            "\n",
            "\n",
            "Who uses sentiment analysis to detect brand sentiment on social media?\n",
            "Businesses\n",
            "\n",
            "\n",
            "What does natural language processing help machines do?\n",
            "Machines process\n",
            "\n",
            "\n",
            "time: 2.53 s (started: 2022-09-27 09:39:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating Distractors Using Wordnet**"
      ],
      "metadata": {
        "id": "nIrGmdzWkViJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "L93XpDE1O6Pw",
        "outputId": "2e91d697-5bf8-4419-898a-96b1a1981a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"lion\"\n",
        "word = word.lower()\n",
        "syns = wn.synsets(word,'n')\n",
        "\n",
        "\n",
        "hypernym = syns[0].hypernyms()\n",
        "print (hypernym)\n",
        "print (hypernym[0].hyponyms())"
      ],
      "metadata": {
        "id": "zIgNZ3ySOmDh",
        "outputId": "388e9644-e5b5-426a-9574-0bfa7459a952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('big_cat.n.01')]\n",
            "[Synset('cheetah.n.01'), Synset('jaguar.n.01'), Synset('leopard.n.02'), Synset('liger.n.01'), Synset('lion.n.01'), Synset('saber-toothed_tiger.n.01'), Synset('snow_leopard.n.01'), Synset('tiger.n.02'), Synset('tiglon.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distractors_wordnet(syn,word):\n",
        "    distractors=[]\n",
        "    word= word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0: \n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        #print (\"name \",name, \" word\",orig_word)\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\",\" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "\n",
        "original_word = \"lion\"\n",
        "synset_to_use = wn.synsets(original_word,'n')[0]\n",
        "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "print (\"original word: \",original_word.capitalize())\n",
        "print (distractors_calculated)\n",
        "\n",
        "original_word = \"green\"\n",
        "synset_to_use = wn.synsets(original_word,'n')[0]\n",
        "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "print (\"\\noriginal word: \",original_word.capitalize())\n",
        "print (distractors_calculated)\n"
      ],
      "metadata": {
        "id": "cYCWHeMHcp2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351304d6-7e6c-4171-c0dc-4c083f2caa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original word:  Lion\n",
            "['Cheetah', 'Jaguar', 'Leopard', 'Liger', 'Saber-toothed Tiger', 'Snow Leopard', 'Tiger', 'Tiglon']\n",
            "\n",
            "original word:  Green\n",
            "['Blond', 'Blue', 'Brown', 'Complementary Color', 'Olive', 'Orange', 'Pastel', 'Pink', 'Purple', 'Red', 'Salmon', 'Yellow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_word = \"cricket\"\n",
        "\n",
        "syns = wn.synsets(original_word,'n')\n",
        "\n",
        "for syn in syns:\n",
        "  print (syn, \": \",syn.definition(),\"\\n\" )\n",
        "\n",
        "\n",
        "synset_to_use = wn.synsets(original_word,'n')[0]\n",
        "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "print (\"\\noriginal word: \",original_word.capitalize())\n",
        "print (distractors_calculated)\n",
        "\n",
        "\n",
        "original_word = \"cricket\"\n",
        "synset_to_use = wn.synsets(original_word,'n')[1]\n",
        "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
        "\n",
        "print (\"\\noriginal word: \",original_word.capitalize())\n",
        "print (distractors_calculated)"
      ],
      "metadata": {
        "id": "YpfTfFkUdDl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd297f48-7545-4a1e-a19e-4dbb1f1dc190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('cricket.n.01') :  leaping insect; male makes chirping noises by rubbing the forewings together \n",
            "\n",
            "Synset('cricket.n.02') :  a game played with a ball and bat by two teams of 11 players; teams take turns trying to score runs \n",
            "\n",
            "\n",
            "original word:  Cricket\n",
            "['Grasshopper']\n",
            "\n",
            "original word:  Cricket\n",
            "['Ball Game', 'Field Hockey', 'Football', 'Hurling', 'Lacrosse', 'Polo', 'Pushball', 'Ultimate Frisbee']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating Distractors Using Sence2Vec**"
      ],
      "metadata": {
        "id": "Up5hW46Hkx13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sense2vec==1.0.2"
      ],
      "metadata": {
        "id": "pTdztvuGPeH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cb704c-6f12-41d7-9a29-acb63566879d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sense2vec==1.0.2\n",
            "  Downloading sense2vec-1.0.2.tar.gz (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.0,>=2.2.3\n",
            "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (2.4.4)\n",
            "Requirement already satisfied: catalogue>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (1.21.6)\n",
            "Requirement already satisfied: importlib_metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue>=0.0.4->sense2vec==1.0.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue>=0.0.4->sense2vec==1.0.2) (3.8.1)\n",
            "Collecting catalogue>=0.0.4\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (4.64.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (0.7.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.23.0)\n",
            "Collecting srsly>=0.2.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 74.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.0.8)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.0.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2022.6.15)\n",
            "Building wheels for collected packages: sense2vec\n",
            "  Building wheel for sense2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sense2vec: filename=sense2vec-1.0.2-py2.py3-none-any.whl size=35011 sha256=1828dd4ddd09db55c05ef3fee73e7fa86835fa94b0f32261c53a522f9c966096\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/d3/93/fe8e871b410c5456a7b06be0f154ad6bab298462471551f39d\n",
            "Successfully built sense2vec\n",
            "Installing collected packages: srsly, plac, catalogue, thinc, spacy, sense2vec\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.7 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-1.0.0 plac-1.1.3 sense2vec-1.0.2 spacy-2.3.7 srsly-1.0.5 thinc-7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz"
      ],
      "metadata": {
        "id": "PtxkMgOiPePv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068c99a1-4232-4ae2-9ac2-386eee2020b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-27 09:41:19--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220927T094119Z&X-Amz-Expires=300&X-Amz-Signature=92d30c78e5c69dc4578f6b4abadd08adf5e30594fc76cd9d4c0f880dc2494a78&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-27 09:41:19--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220927T094119Z&X-Amz-Expires=300&X-Amz-Signature=92d30c78e5c69dc4578f6b4abadd08adf5e30594fc76cd9d4c0f880dc2494a78&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600444501 (573M) [application/octet-stream]\n",
            "Saving to: ‘s2v_reddit_2015_md.tar.gz’\n",
            "\n",
            "s2v_reddit_2015_md. 100%[===================>] 572.63M   130MB/s    in 4.5s    \n",
            "\n",
            "2022-09-27 09:41:24 (126 MB/s) - ‘s2v_reddit_2015_md.tar.gz’ saved [600444501/600444501]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ],
      "metadata": {
        "id": "fk9FzcJ2Pl1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230c9c4c-4171-4c40-97c4-c5dcad08289a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./._s2v_old\n",
            "./s2v_old/\n",
            "./s2v_old/._freqs.json\n",
            "./s2v_old/freqs.json\n",
            "./s2v_old/._vectors\n",
            "./s2v_old/vectors\n",
            "./s2v_old/._cfg\n",
            "./s2v_old/cfg\n",
            "./s2v_old/._strings.json\n",
            "./s2v_old/strings.json\n",
            "./s2v_old/._key2row\n",
            "./s2v_old/key2row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sense2vec import Sense2Vec\n",
        "s2v = Sense2Vec().from_disk('s2v_old')"
      ],
      "metadata": {
        "id": "iuE5XDqwPp_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"Narendra Modi\"\n",
        "word = word.lower()\n",
        "word = word.replace(\" \", \"_\")\n",
        "\n",
        "print (\"word \",word)\n",
        "\n",
        "sense = s2v.get_best_sense(word)\n",
        "\n",
        "print (\"Best sense \",sense)\n",
        "most_similar = s2v.most_similar(sense, n=12)\n",
        "\n",
        "\n",
        "print (most_similar)"
      ],
      "metadata": {
        "id": "ExiUZBjWOmWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df92289-2bdd-4d86-fb80-c17e4c8b4325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word  narendra_modi\n",
            "Best sense  Narendra_Modi|PERSON\n",
            "[('Modi|PERSON', 0.8089), ('Rahul_Gandhi|PERSON', 0.7973), ('Benjamin_Netanyahu|PERSON', 0.7767), ('Manmohan_Singh|PERSON', 0.7692), ('Modi|ORG', 0.7642), ('Prime_Minister|NOUN', 0.7542), ('PM_Modi|NOUN', 0.7519), ('David_Cameron|PERSON', 0.75), ('Nehru|NORP', 0.7486), ('Kejriwal|PERSON', 0.748), ('Kejriwal|GPE', 0.7476), ('prime_minister|NOUN', 0.7475)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distractors = []\n",
        "\n",
        "for each_word in most_similar:\n",
        "  append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
        "  if append_word.lower() != word:\n",
        "      distractors.append(append_word.title())\n",
        "\n",
        "print (distractors)"
      ],
      "metadata": {
        "id": "VOmsrdkMQjIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a32386-fdf9-4c4e-a00d-5e0df4b7a858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sarah Palin', 'Mitt Romney', 'Barrack Obama', 'Bill Clinton', 'Oprah', 'Paris Hilton', 'Palin', 'Oprah Winfrey', 'Stephen Colbert', 'Oprah', 'Hilary Clinton', 'Herman Cain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "def sense2vec_get_words(word,s2v):\n",
        "    output = []\n",
        "    word = word.lower()\n",
        "    word = word.replace(\" \", \"_\")\n",
        "\n",
        "    sense = s2v.get_best_sense(word)\n",
        "    most_similar = s2v.most_similar(sense, n=20)\n",
        "\n",
        "    # print (\"most_similar \",most_similar)\n",
        "\n",
        "    for each_word in most_similar:\n",
        "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
        "        if append_word.lower() != word:\n",
        "            output.append(append_word.title())\n",
        "\n",
        "    out = list(OrderedDict.fromkeys(output))\n",
        "    return out\n",
        "\n",
        "word = \"USA\"\n",
        "distractors = sense2vec_get_words(word,s2v)\n",
        "\n",
        "print (\"Distractors for \",word, \" : \")\n",
        "print (distractors)"
      ],
      "metadata": {
        "id": "WDRog0Q1dXw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ef0a57-52b8-4e56-9aa0-0af2ccb7de08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distractors for  USA  : \n",
            "['Usa.', 'U.S', 'U.S.', 'Us.', 'Us', 'America', 'Canada', 'U.S.A', 'United States', 'Country', 'Only Country', 'Mexico', 'Other Countries', 'U.K.', 'Europe', 'U.S.A.']\n"
          ]
        }
      ]
    }
  ]
}